---
title: "Multiple linear regression"
author: ""
output:
  pdf_document: default
  html_document:
    includes:
      in_header: header.html
    css: ./lab.css
    highlight: pygments
    theme: cerulean
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, message = FALSE, warning = FALSE)
```

```{r load-packages, message=FALSE}
library(tidyverse)
library(openintro)
library(GGally)
evals <- evals
```

### Exercise 1

#### Is this an observational study or an experiment? The original research question posed in the paper is whether beauty leads directly to the differences in course evaluations. Given the study design, is it possible to answer this question as it is phrased? If not, rephrase the question.

> I would say this is an observational study given there's no control or test group to compare each other against; we're looking solelly at observations found in the data rather than experimenting with multiple groups. The question itself is hard to answer as phrased since even if there's a correlation between the two variables, that does not mean causation. An easier/more manageable question to ask is whether there is a relationship between a professor's attractiveness and course evaluation scores.

### Exercise 2

#### Describe the distribution of `score`. Is the distribution skewed? What does that tell you about how students rate courses? Is this what you expected to see? Why, or why not?
```{r ex2}
hist(evals$score, main = "Scores Histogram", xlab = "Score")
```

> The distribution is skewed to the left which means that students rated courses more positively. I would expect a more normal distribution, however this means that the samples taken are likely due to students either not being super critical or the professors are pretty fair and have a good reputation.

### Exercise 3

#### Excluding `score`, select two other variables and describe their relationship with each other using an appropriate visualization.

```{r ex3}
boxplot(evals$age ~ evals$gender, main = "Boxplot of Gender vs. Age", ylab = "Age", xlab = "Gender")
```

> It seems like male professors tend to be older than female ones with the distribution of male professors being much higher, ~45-60, with females being more towards ~39-52. 

### Exercise 4

#### Replot the scatterplot, but this time use `geom_jitter` as your layer. What was misleading about the initial scatterplot?

```{r scatter-score-bty_avg}
ggplot(data = evals, aes(x = bty_avg, y = score)) +
  geom_point()
```

```{r scatter-score-bty_avg-jitter}
ggplot(data = evals, aes(x = bty_avg, y = score)) +
  geom_jitter()
```

> The first one is misleading as it makes it seem like there are less points than there actual are since it overlaps its points if they're on the same coordinate, but `geom_jitter` adds some noise and gives more insight into how many pieces of data there actually are. 

### Exercise 5

#### Let's see if the apparent trend in the plot is something more than natural variation. Fit a linear model called `m_bty` to predict average professor score by average beauty rating. Write out the equation for the linear model and interpret the slope. Is average beauty score a statistically significant predictor? Does it appear to be a practically significant predictor?

```{r ex5}
m_bty <- lm(evals$score ~ evals$bty_avg)
plot(jitter(evals$score) ~ jitter(evals$bty_avg))
abline(m_bty)
summary(m_bty)
```
    
> \[
  \hat{y} = 3.88034 + 0.06664 \times bty\_avg
\]

> This formula can be interpreted that for every increase in 1 for `bty_avg`, `score` increases by 0.06664. The p-value is statistically significant as it's quite small (0.0000508), but given the impact is so small (0.067), it's not a *practically* significant predictor.

### Exercise 6

#### Use residual plots to evaluate whether the conditions of least squares regression are reasonable. Provide plots and comments for each one (see the Simple Regression Lab for a reminder of how to make these).

```{r ex6}
ggplot(data = m_bty, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")
```

```{r hist-res}
ggplot(data = m_bty, aes(x = .resid)) +
  geom_histogram(binwidth = 0.5) +
  xlab("Residuals")
```

```{r qq-res}
ggplot(data = m_bty, aes(sample = .resid)) +
  stat_qq()
```

- Independence: We don't have too much information on how the sample was taken, so we'll assume independence for this.
- Linear Relationship: Visually, the data looks linear with a slightly positive relationship.
- Constant Variance: As shown in the Fitted Values Residuals plot, there does seem to be constant variance.
- Normality of Residuals: The histogram shows a slightly left skew and the qq plot shows a short, right tail; as short tail is relatively harmless, we can move forward and assume normality.

### Exercise 7

#### P-values and parameter estimates should only be trusted if the conditions for the regression are reasonable. Verify that the conditions for this model are reasonable using diagnostic plots.

```{r scatter-score-bty_avg_pic-color}
m_bty_gen <- lm(score ~ bty_avg + gender, data = evals)
summary(m_bty_gen)
```

```{r ex7}
ggplot(data = m_bty_gen, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")
```

```{r hist-res-ex7}
ggplot(data = m_bty_gen, aes(x = .resid)) +
  geom_histogram(binwidth = 0.5) +
  xlab("Residuals")
```

```{r qq-res-ex7}
ggplot(data = m_bty_gen, aes(sample = .resid)) +
  stat_qq()
```

> The data looks super similar to the previous model using only `bty_avg` so I would say that the conditions for this model are reasonable for the same reasons as exercise 6.

### Exercise 8

#### Is `bty_avg` still a significant predictor of `score`? Has the addition of `gender` to the model changed the parameter estimate for `bty_avg`?

```{r ex8}
summary(m_bty_gen)
```

> The p-value for `bty_avg` is lower than the previous model so it's still a statistically significant predictor. The value itself at 0.07416 is slightly higher than the 0.06664, however overall is still pretty small. Adding `gendermale` to the model has made it slightly higher it seems.

### Exercise 9

#### What is the equation of the line corresponding to those with color pictures? (*Hint:* For those with color pictures, the parameter estimate is multiplied by 1.) For two professors who received the same beauty rating, which color picture tends to have the higher course evaluation score?

```{r twoLines}
ggplot(data = evals, aes(x = bty_avg, y = score, color = pic_color)) +
 geom_smooth(method = "lm", formula = y ~ x, se = FALSE)
```

```{r ex9}
summary(lm(score ~ bty_avg + pic_color, data = evals))
```

> \[
  \hat{y} = 4.06318 + 0.05548 \times bty\_avg - 0.16059
\]

> For two professors who received the same beauty rating, those with a black and white color have a higher course evaluation score.

### Exercise 10

#### Create a new model called `m_bty_rank` with `gender` removed and `rank` added in. How does R appear to handle categorical variables that have more than two levels? Note that the rank variable has three levels: `teaching`, `tenure track`, `tenured`.

```{r ex10}
m_bty_rank <- lm(score ~ bty_avg + rank, data = evals)
summary(m_bty_rank)
```

> R seems to have added two variables for a categorical variable with three values; they left out `teaching`, but there is one for `ranktenure track` and `ranktenured`. 

### Exercise 11 

#### Which variable would you expect to have the highest p-value in this model? Why? *Hint:* Think about which variable would you expect to not have any association with the professor score.

We will start with a full model that predicts professor score based on rank, gender, ethnicity, language of the university where they got their degree, age, proportion of students that filled out evaluations, class size, course level, number of professors, number of credits, average beauty rating, outfit, and picture color.

> I think variables that *wouldn't* have any association with score would be number of credits, age, and proportion of students that filled out evaluations. I believe these factors don't impact how a student scores a course. Perhaps the one that would have the highest p-value is `rank` given the more tenured professors have more experience so they're likely to be pretty good at teaching.

### Exercise 12

```{r m_full, tidy = FALSE}
m_full <- lm(score ~ rank + gender + ethnicity + language + age + cls_perc_eval 
             + cls_students + cls_level + cls_profs + cls_credits + bty_avg 
             + pic_outfit + pic_color, data = evals)
summary(m_full)
```

#### Check your suspicions from the previous exercise. Include the model output in your response.

> \[
  \hat{y} = 4.0952141 - 0.1475932 \times ranktenure track - 0.0973378 \times ranktenured + 0.2109481 \times gendermale + 0.1234929 \times ethnicitynotminority - 0.2298112 \times languagenonenglish - 0.0090072 \times age + 0.0053272 \times cls\_perc\_eval + 0.0004546 \times cls\_students + 0.0605140 \times cls\_levelupper - 0.0146619 \times cls\_profssingle + 0.5020432 \times cls\_creditsone credit + 0.0400333 \times bty\_avg - 0.1126817 \times pic\_outfitnotformal - 0.2172630 \times pic\_colorcolor
\]

> I was wrong here as number of credits, age, and proportion of students that filled out evaluations are all pretty significant with low p-values. The value with the least significant p-value was `cls_profssingle` (number of professors) while the one with the lowest p-value was `cls_creditsone credit`.

### Exercise 13

#### Interpret the coefficient associated with the ethnicity variable.

> The coefficient with `ethnicity` is saying that if the professor is not a minority and all things are equal, there is a slightly positive impact to `score`.

### Exercise 14

#### Drop the variable with the highest p-value and re-fit the model. Did the coefficients and significance of the other explanatory variables change? (One of the things that makes multiple regression interesting is that coefficient estimates depend on the other variables that are included in the model.) If not, what does this say about whether or not the dropped variable was collinear with the other explanatory variables?

```{r ex14, tidy = FALSE}
m_full_no_cls_profs <- lm(score ~ rank + gender + ethnicity + language + age + cls_perc_eval 
             + cls_students + cls_level + cls_credits + bty_avg 
             + pic_outfit + pic_color, data = evals)
summary(m_full_no_cls_profs)
```

> The coefficients do change slightly, however the impact is very small and it also didn't affect the significance of most of the other variables. This means that the dropped variable was not that collinear with the other variables.

### Exercise 15

#### Using backward-selection and p-value as the selection criterion, determine the best model. You do not need to show all steps in your answer, just the output for the final model. Also, write out the linear model for predicting score based on the final model you settle on.

```{r ex15, tidy = FALSE}
m_best <- lm(score ~ gender + ethnicity + language + age + cls_perc_eval 
             + cls_credits + bty_avg + pic_color, data = evals)
summary(m_best)
```

> \[
  \hat{y} = 3.771922 + 0.207112 \times gendermale + 0.167872 \times ethnicitynotminority - 0.206178 \times languagenonenglish - 0.006046 \times age + 0.004656 \times cls\_perc\_eval + 0.505306 \times cls\_creditsone credit + 0.051069 \times bty\_avg - 0.190579 \times pic\_colorcolor
\]

### Exercise 16

#### Verify that the conditions for this model are reasonable using diagnostic plots.

```{r ex16}
ggplot(data = m_best, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")
```

```{r hist-res-ex16}
ggplot(data = m_best, aes(x = .resid)) +
  geom_histogram(binwidth = 0.5) +
  xlab("Residuals")
```

```{r qq-res-ex16}
ggplot(data = m_best, aes(sample = .resid)) +
  stat_qq()
```

- Independence: We have already assumed this based on previous models.
- Linear Relationship: Visually, we can see a linear relationship.
- Constant Variance: We can see a uniform spread of residuals.
- Normality of Residuals: Residuals are normal across a reasonable range, which is shown by the chart above.

### Exercise 17

#### The original paper describes how these data were gathered by taking a sample of professors from the University of Texas at Austin and including all courses that they have taught. Considering that each row represents a course, could this new information have an impact on any of the conditions of linear regression?

> No, because even a professor teaches multiple classes, courses are independent of each other so evaluation scores from one course is not related to another.

### Exercise 18

#### Based on your final model, describe the characteristics of a professor and course at University of Texas at Austin that would be associated with a high evaluation score.

> A professor and course at the University of Texas at Austin with a high score would be one that has the following qualities:

- The professor is male
- The professor is not an ethnic minority
- The professor got their degree at an English-speaking university
- The professor is young
- A higher % of students filled out the evaluation
- The course is one credit
- The professor has a higher beauty rating
- The professor has a black and white photo

### Exercise 19

#### Would you be comfortable generalizing your conclusions to apply to professors generally (at any university)? Why or why not?

> No, this sample size is not representative of every university or set of professors. It wouldn't make sense to apply this to another university that has a different dominant language for example since English would then not be most common.